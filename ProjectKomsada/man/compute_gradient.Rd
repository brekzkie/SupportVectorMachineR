% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gradient.R
\name{compute_gradient}
\alias{compute_gradient}
\title{Compute the Gradient of the Hinge Loss Function for SVM}
\usage{
compute_gradient(X, y, w, b, lambda = 0.01)
}
\arguments{
\item{X}{A numeric matrix of shape (n_samples, n_features), representing the input data.}

\item{y}{A numeric vector of labels (either -1 or 1), with length equal to `nrow(X)`.}

\item{w}{A numeric vector of weights with length equal to `ncol(X)`.}

\item{b}{A numeric value representing the bias term.}

\item{lambda}{A numeric value representing the regularization strength (default is 0.01).}
}
\value{
A list containing:
\describe{
  \item{dw}{A numeric vector of gradients with respect to the weights.}
  \item{db}{A numeric value representing the gradient with respect to the bias.}
}
}
\description{
This function computes the gradients of the hinge loss function for a linear SVM model.
It calculates the gradients with respect to the weights (`w`) and the bias (`b`), including
the regularization term for weight decay.
}
\details{
The gradient is calculated for each sample, and the total gradient is averaged over
all samples. Additionally, a regularization term is added to the gradient of the weights to penalize
large weight values, as follows:
\deqn{dw = \frac{1}{n} \sum_{i=1}^{n} \nabla_w L_i + 2\lambda w}
where \(\nabla_w L_i\) is the gradient of the hinge loss for each sample and \( \lambda \) is the
regularization parameter.
}
\examples{
X <- matrix(c(2, 3, 4, 5), nrow = 2)
y <- c(1, -1)
w <- c(0.5, -0.3)
b <- 0.1
compute_gradient(X, y, w, b, lambda = 0.01)

}
